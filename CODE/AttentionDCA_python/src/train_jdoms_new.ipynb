{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7522ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences read: 14502\n",
      "Sequences after filtering: 14502\n",
      "Sampling 100000 pairs out of 105146751 total pairs.\n",
      "Mean fraction of identical positions (sampled): 0.3723494608104989\n",
      "Computed theta: 0.32657493241781893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14502/14502 [00:14<00:00, 1025.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3265.70225762244\n",
      "65\n",
      "Using device: cpu\n",
      "not working\n",
      "Epoch 1 - Train Loss: 1821.1635, Val Loss: 1030.4902\n",
      "Epoch 2 - Train Loss: 1178.8903, Val Loss: 713.2452\n",
      "Epoch 3 - Train Loss: 849.4186, Val Loss: 530.0574\n",
      "Epoch 4 - Train Loss: 663.8922, Val Loss: 417.4704\n",
      "Epoch 5 - Train Loss: 551.7207, Val Loss: 346.3014\n",
      "Epoch 6 - Train Loss: 477.9436, Val Loss: 296.6934\n",
      "Epoch 7 - Train Loss: 426.0585, Val Loss: 260.6187\n",
      "Epoch 8 - Train Loss: 387.3178, Val Loss: 233.0294\n",
      "Epoch 9 - Train Loss: 357.2313, Val Loss: 211.1547\n",
      "Epoch 10 - Train Loss: 332.8428, Val Loss: 193.0253\n",
      "Epoch 11 - Train Loss: 312.3270, Val Loss: 177.5833\n",
      "Epoch 12 - Train Loss: 294.8784, Val Loss: 164.3027\n",
      "Epoch 13 - Train Loss: 279.7718, Val Loss: 152.7081\n",
      "Epoch 14 - Train Loss: 266.5383, Val Loss: 142.5303\n",
      "Epoch 15 - Train Loss: 254.8197, Val Loss: 133.5290\n",
      "Epoch 16 - Train Loss: 244.4657, Val Loss: 125.5406\n",
      "Epoch 17 - Train Loss: 235.1624, Val Loss: 118.3538\n",
      "Epoch 18 - Train Loss: 226.8832, Val Loss: 111.8618\n",
      "Epoch 19 - Train Loss: 219.2706, Val Loss: 106.0087\n",
      "Epoch 20 - Train Loss: 212.5233, Val Loss: 100.6944\n",
      "Epoch 21 - Train Loss: 206.2139, Val Loss: 95.8236\n",
      "Epoch 22 - Train Loss: 200.4872, Val Loss: 91.3462\n",
      "Epoch 23 - Train Loss: 195.2127, Val Loss: 87.2435\n",
      "Epoch 24 - Train Loss: 190.3590, Val Loss: 83.4228\n",
      "Epoch 25 - Train Loss: 185.8140, Val Loss: 79.8834\n",
      "Epoch 26 - Train Loss: 181.6002, Val Loss: 76.6037\n",
      "Epoch 27 - Train Loss: 177.7263, Val Loss: 73.5670\n",
      "Epoch 28 - Train Loss: 174.0534, Val Loss: 70.7182\n",
      "Epoch 29 - Train Loss: 170.6381, Val Loss: 68.0488\n",
      "Epoch 30 - Train Loss: 167.4257, Val Loss: 65.5395\n",
      "Epoch 31 - Train Loss: 164.4832, Val Loss: 63.2015\n",
      "Epoch 32 - Train Loss: 161.6003, Val Loss: 61.0008\n",
      "Epoch 33 - Train Loss: 158.8504, Val Loss: 58.9435\n",
      "Epoch 34 - Train Loss: 156.3668, Val Loss: 56.9833\n",
      "Epoch 35 - Train Loss: 154.0668, Val Loss: 55.1402\n",
      "Epoch 36 - Train Loss: 151.7673, Val Loss: 53.4029\n",
      "Epoch 37 - Train Loss: 149.7312, Val Loss: 51.7623\n",
      "Epoch 38 - Train Loss: 147.6628, Val Loss: 50.2140\n",
      "Epoch 39 - Train Loss: 145.7160, Val Loss: 48.7385\n",
      "Epoch 40 - Train Loss: 143.8781, Val Loss: 47.3505\n",
      "Epoch 41 - Train Loss: 142.2129, Val Loss: 46.0184\n",
      "Epoch 42 - Train Loss: 140.5355, Val Loss: 44.7785\n",
      "Epoch 43 - Train Loss: 138.9298, Val Loss: 43.5907\n",
      "Epoch 44 - Train Loss: 137.4621, Val Loss: 42.4397\n",
      "Epoch 45 - Train Loss: 135.9911, Val Loss: 41.3650\n",
      "Epoch 46 - Train Loss: 134.7358, Val Loss: 40.3398\n",
      "Epoch 47 - Train Loss: 133.3633, Val Loss: 39.3542\n",
      "Epoch 48 - Train Loss: 132.1293, Val Loss: 38.4253\n",
      "Epoch 49 - Train Loss: 130.9535, Val Loss: 37.5360\n",
      "Epoch 50 - Train Loss: 129.8356, Val Loss: 36.6758\n",
      "Epoch 51 - Train Loss: 128.6630, Val Loss: 35.8716\n",
      "Epoch 52 - Train Loss: 127.7143, Val Loss: 35.0805\n",
      "Epoch 53 - Train Loss: 126.6063, Val Loss: 34.3382\n",
      "Epoch 54 - Train Loss: 125.7170, Val Loss: 33.6168\n",
      "Epoch 55 - Train Loss: 124.7561, Val Loss: 32.9366\n",
      "Epoch 56 - Train Loss: 123.8323, Val Loss: 32.2741\n",
      "Epoch 57 - Train Loss: 122.9613, Val Loss: 31.6370\n",
      "Epoch 58 - Train Loss: 122.1558, Val Loss: 31.0499\n",
      "Epoch 59 - Train Loss: 121.3011, Val Loss: 30.4662\n",
      "Epoch 60 - Train Loss: 120.5442, Val Loss: 29.9064\n",
      "Epoch 61 - Train Loss: 119.7927, Val Loss: 29.3717\n",
      "Epoch 62 - Train Loss: 119.1076, Val Loss: 28.8554\n",
      "Epoch 63 - Train Loss: 118.3814, Val Loss: 28.3628\n",
      "Epoch 64 - Train Loss: 117.7701, Val Loss: 27.8889\n",
      "Epoch 65 - Train Loss: 117.0283, Val Loss: 27.4209\n",
      "Epoch 66 - Train Loss: 116.4481, Val Loss: 26.9886\n",
      "Epoch 67 - Train Loss: 115.8126, Val Loss: 26.5670\n",
      "Epoch 68 - Train Loss: 115.1999, Val Loss: 26.1437\n",
      "Epoch 69 - Train Loss: 114.7750, Val Loss: 25.7637\n",
      "Epoch 70 - Train Loss: 114.1103, Val Loss: 25.3777\n",
      "Epoch 71 - Train Loss: 113.6329, Val Loss: 25.0244\n",
      "Epoch 72 - Train Loss: 113.1254, Val Loss: 24.6566\n",
      "Epoch 73 - Train Loss: 112.5661, Val Loss: 24.3223\n",
      "Epoch 74 - Train Loss: 112.1029, Val Loss: 23.9864\n",
      "Epoch 75 - Train Loss: 111.5459, Val Loss: 23.6816\n",
      "Epoch 76 - Train Loss: 111.2588, Val Loss: 23.3684\n",
      "Epoch 77 - Train Loss: 110.7171, Val Loss: 23.0797\n",
      "Epoch 78 - Train Loss: 110.3118, Val Loss: 22.7887\n",
      "Epoch 79 - Train Loss: 109.9359, Val Loss: 22.4920\n",
      "Epoch 80 - Train Loss: 109.5411, Val Loss: 22.2473\n",
      "Epoch 81 - Train Loss: 109.0442, Val Loss: 21.9685\n",
      "Epoch 82 - Train Loss: 108.7571, Val Loss: 21.7355\n",
      "Epoch 83 - Train Loss: 108.3060, Val Loss: 21.4871\n",
      "Epoch 84 - Train Loss: 107.9898, Val Loss: 21.2545\n",
      "Epoch 85 - Train Loss: 107.5795, Val Loss: 21.0245\n",
      "Epoch 86 - Train Loss: 107.2791, Val Loss: 20.8049\n",
      "Epoch 87 - Train Loss: 106.8884, Val Loss: 20.5962\n",
      "Epoch 88 - Train Loss: 106.5186, Val Loss: 20.3989\n",
      "Epoch 89 - Train Loss: 106.3108, Val Loss: 20.1981\n",
      "Epoch 90 - Train Loss: 105.9990, Val Loss: 20.0059\n",
      "Epoch 91 - Train Loss: 105.6808, Val Loss: 19.8176\n",
      "Epoch 92 - Train Loss: 105.4046, Val Loss: 19.6284\n",
      "Epoch 93 - Train Loss: 105.0775, Val Loss: 19.4492\n",
      "Epoch 94 - Train Loss: 104.8030, Val Loss: 19.2877\n",
      "Epoch 95 - Train Loss: 104.5177, Val Loss: 19.1208\n",
      "Epoch 96 - Train Loss: 104.2702, Val Loss: 18.9757\n",
      "Epoch 97 - Train Loss: 103.9830, Val Loss: 18.8164\n",
      "Epoch 98 - Train Loss: 103.7346, Val Loss: 18.6687\n",
      "Epoch 99 - Train Loss: 103.5177, Val Loss: 18.5273\n",
      "Epoch 100 - Train Loss: 103.1992, Val Loss: 18.3911\n",
      "Epoch 101 - Train Loss: 103.0780, Val Loss: 18.2193\n",
      "Epoch 102 - Train Loss: 102.8212, Val Loss: 18.1189\n",
      "Epoch 103 - Train Loss: 102.5414, Val Loss: 17.9710\n",
      "Epoch 104 - Train Loss: 102.3676, Val Loss: 17.8593\n",
      "Epoch 105 - Train Loss: 102.1665, Val Loss: 17.7290\n",
      "Epoch 106 - Train Loss: 101.8764, Val Loss: 17.6208\n",
      "Epoch 107 - Train Loss: 101.7159, Val Loss: 17.5063\n",
      "Epoch 108 - Train Loss: 101.5860, Val Loss: 17.3911\n",
      "Epoch 109 - Train Loss: 101.2972, Val Loss: 17.2849\n",
      "Epoch 110 - Train Loss: 101.1036, Val Loss: 17.1750\n",
      "Epoch 111 - Train Loss: 100.9930, Val Loss: 17.0911\n",
      "Epoch 112 - Train Loss: 100.6901, Val Loss: 16.9661\n",
      "Epoch 113 - Train Loss: 100.5712, Val Loss: 16.8993\n",
      "Epoch 114 - Train Loss: 100.3931, Val Loss: 16.7962\n",
      "Epoch 115 - Train Loss: 100.2445, Val Loss: 16.7125\n",
      "Epoch 116 - Train Loss: 100.0724, Val Loss: 16.6415\n",
      "Epoch 117 - Train Loss: 99.9014, Val Loss: 16.5368\n",
      "Epoch 118 - Train Loss: 99.7210, Val Loss: 16.4653\n",
      "Epoch 119 - Train Loss: 99.5715, Val Loss: 16.3745\n",
      "Epoch 120 - Train Loss: 99.4424, Val Loss: 16.2972\n",
      "Epoch 121 - Train Loss: 99.2641, Val Loss: 16.2193\n",
      "Epoch 122 - Train Loss: 99.1874, Val Loss: 16.1512\n",
      "Epoch 123 - Train Loss: 98.9495, Val Loss: 16.0816\n",
      "Epoch 124 - Train Loss: 98.8226, Val Loss: 16.0177\n",
      "Epoch 125 - Train Loss: 98.7426, Val Loss: 15.9527\n",
      "Epoch 126 - Train Loss: 98.5767, Val Loss: 15.8851\n",
      "Epoch 127 - Train Loss: 98.3591, Val Loss: 15.8047\n",
      "Epoch 128 - Train Loss: 98.3613, Val Loss: 15.7717\n",
      "Epoch 129 - Train Loss: 98.1663, Val Loss: 15.6941\n",
      "Epoch 130 - Train Loss: 97.9752, Val Loss: 15.6538\n",
      "Epoch 131 - Train Loss: 97.9132, Val Loss: 15.5888\n",
      "Epoch 132 - Train Loss: 97.7604, Val Loss: 15.5351\n",
      "Epoch 133 - Train Loss: 97.6792, Val Loss: 15.4751\n",
      "Epoch 134 - Train Loss: 97.5791, Val Loss: 15.4435\n",
      "Epoch 135 - Train Loss: 97.4920, Val Loss: 15.3837\n",
      "Epoch 136 - Train Loss: 97.3851, Val Loss: 15.3293\n",
      "Epoch 137 - Train Loss: 97.3126, Val Loss: 15.3028\n",
      "Epoch 138 - Train Loss: 97.1811, Val Loss: 15.2345\n",
      "Epoch 139 - Train Loss: 97.0905, Val Loss: 15.2033\n",
      "Epoch 140 - Train Loss: 97.0299, Val Loss: 15.1603\n",
      "Epoch 141 - Train Loss: 96.8728, Val Loss: 15.1202\n",
      "Epoch 142 - Train Loss: 96.7458, Val Loss: 15.0701\n",
      "Epoch 143 - Train Loss: 96.6989, Val Loss: 15.0391\n",
      "Epoch 144 - Train Loss: 96.6537, Val Loss: 15.0140\n",
      "Epoch 145 - Train Loss: 96.5316, Val Loss: 14.9624\n",
      "Epoch 146 - Train Loss: 96.4004, Val Loss: 14.9334\n",
      "Epoch 147 - Train Loss: 96.3474, Val Loss: 14.9035\n",
      "Epoch 148 - Train Loss: 96.2517, Val Loss: 14.8668\n",
      "Epoch 149 - Train Loss: 96.1092, Val Loss: 14.8310\n",
      "Epoch 150 - Train Loss: 96.0702, Val Loss: 14.8007\n",
      "Epoch 151 - Train Loss: 95.9772, Val Loss: 14.7570\n",
      "Epoch 152 - Train Loss: 95.8511, Val Loss: 14.7474\n",
      "Epoch 153 - Train Loss: 95.8256, Val Loss: 14.7160\n",
      "Epoch 154 - Train Loss: 95.7069, Val Loss: 14.6814\n",
      "Epoch 155 - Train Loss: 95.6233, Val Loss: 14.6658\n",
      "Epoch 156 - Train Loss: 95.5425, Val Loss: 14.6308\n",
      "Epoch 157 - Train Loss: 95.5186, Val Loss: 14.6178\n",
      "Epoch 158 - Train Loss: 95.4966, Val Loss: 14.5735\n",
      "Epoch 159 - Train Loss: 95.3794, Val Loss: 14.5662\n",
      "Epoch 160 - Train Loss: 95.3386, Val Loss: 14.5229\n",
      "Epoch 161 - Train Loss: 95.2425, Val Loss: 14.5159\n",
      "Epoch 162 - Train Loss: 95.2108, Val Loss: 14.4936\n",
      "Epoch 163 - Train Loss: 95.1101, Val Loss: 14.4734\n",
      "Epoch 164 - Train Loss: 95.0817, Val Loss: 14.4437\n",
      "Epoch 165 - Train Loss: 94.9759, Val Loss: 14.4405\n",
      "Epoch 166 - Train Loss: 94.9223, Val Loss: 14.3983\n",
      "Epoch 167 - Train Loss: 94.8530, Val Loss: 14.4014\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 168 - Train Loss: 94.8307, Val Loss: 14.3582\n",
      "Epoch 169 - Train Loss: 94.8265, Val Loss: 14.3584\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 170 - Train Loss: 94.7138, Val Loss: 14.3337\n",
      "Epoch 171 - Train Loss: 94.6651, Val Loss: 14.3121\n",
      "Epoch 172 - Train Loss: 94.6280, Val Loss: 14.2993\n",
      "Epoch 173 - Train Loss: 94.5456, Val Loss: 14.2793\n",
      "Epoch 174 - Train Loss: 94.4722, Val Loss: 14.2781\n",
      "Epoch 175 - Train Loss: 94.4964, Val Loss: 14.2594\n",
      "Epoch 176 - Train Loss: 94.4199, Val Loss: 14.2519\n",
      "Epoch 177 - Train Loss: 94.3386, Val Loss: 14.2282\n",
      "Epoch 178 - Train Loss: 94.2877, Val Loss: 14.2235\n",
      "Epoch 179 - Train Loss: 94.2728, Val Loss: 14.2119\n",
      "Epoch 180 - Train Loss: 94.2344, Val Loss: 14.1802\n",
      "Epoch 181 - Train Loss: 94.1050, Val Loss: 14.1756\n",
      "Epoch 182 - Train Loss: 94.0558, Val Loss: 14.1504\n",
      "Epoch 183 - Train Loss: 94.0650, Val Loss: 14.1496\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 184 - Train Loss: 94.0034, Val Loss: 14.1296\n",
      "Epoch 185 - Train Loss: 93.9697, Val Loss: 14.1449\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 186 - Train Loss: 93.9793, Val Loss: 14.1150\n",
      "Epoch 187 - Train Loss: 93.9017, Val Loss: 14.1080\n",
      "Epoch 188 - Train Loss: 93.8261, Val Loss: 14.0994\n",
      "Epoch 189 - Train Loss: 93.7913, Val Loss: 14.0890\n",
      "Epoch 190 - Train Loss: 93.7438, Val Loss: 14.0747\n",
      "Epoch 191 - Train Loss: 93.6962, Val Loss: 14.0693\n",
      "Epoch 192 - Train Loss: 93.6869, Val Loss: 14.0502\n",
      "Epoch 193 - Train Loss: 93.6450, Val Loss: 14.0548\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 194 - Train Loss: 93.6047, Val Loss: 14.0334\n",
      "Epoch 195 - Train Loss: 93.6038, Val Loss: 14.0402\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 196 - Train Loss: 93.5081, Val Loss: 14.0279\n",
      "Epoch 197 - Train Loss: 93.4187, Val Loss: 14.0122\n",
      "Epoch 198 - Train Loss: 93.3858, Val Loss: 14.0103\n",
      "Epoch 199 - Train Loss: 93.3544, Val Loss: 13.9987\n",
      "Epoch 200 - Train Loss: 93.3299, Val Loss: 14.0008\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 201 - Train Loss: 93.3427, Val Loss: 13.9920\n",
      "Epoch 202 - Train Loss: 93.2900, Val Loss: 13.9959\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 203 - Train Loss: 93.2455, Val Loss: 13.9727\n",
      "Epoch 204 - Train Loss: 93.2521, Val Loss: 13.9586\n",
      "Epoch 205 - Train Loss: 93.1975, Val Loss: 13.9704\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 206 - Train Loss: 93.2045, Val Loss: 13.9655\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 207 - Train Loss: 93.0625, Val Loss: 13.9535\n",
      "Epoch 208 - Train Loss: 93.0530, Val Loss: 13.9497\n",
      "Epoch 209 - Train Loss: 93.0861, Val Loss: 13.9419\n",
      "Epoch 210 - Train Loss: 93.0408, Val Loss: 13.9315\n",
      "Epoch 211 - Train Loss: 92.9779, Val Loss: 13.9334\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 212 - Train Loss: 92.9792, Val Loss: 13.9377\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 213 - Train Loss: 92.8963, Val Loss: 13.8976\n",
      "Epoch 214 - Train Loss: 92.9465, Val Loss: 13.9173\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 215 - Train Loss: 92.9029, Val Loss: 13.9108\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 216 - Train Loss: 92.8969, Val Loss: 13.8947\n",
      "Epoch 217 - Train Loss: 92.8533, Val Loss: 13.9121\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 218 - Train Loss: 92.8104, Val Loss: 13.8843\n",
      "Epoch 219 - Train Loss: 92.7537, Val Loss: 13.9140\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 220 - Train Loss: 92.7569, Val Loss: 13.8726\n",
      "Epoch 221 - Train Loss: 92.7206, Val Loss: 13.8837\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 222 - Train Loss: 92.6765, Val Loss: 13.8750\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 223 - Train Loss: 92.6571, Val Loss: 13.8679\n",
      "Epoch 224 - Train Loss: 92.6378, Val Loss: 13.8822\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 225 - Train Loss: 92.6290, Val Loss: 13.8718\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 226 - Train Loss: 92.6133, Val Loss: 13.8513\n",
      "Epoch 227 - Train Loss: 92.5614, Val Loss: 13.8777\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 228 - Train Loss: 92.5176, Val Loss: 13.8540\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 229 - Train Loss: 92.5099, Val Loss: 13.8717\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 230 - Train Loss: 92.5023, Val Loss: 13.8419\n",
      "Epoch 231 - Train Loss: 92.4282, Val Loss: 13.8589\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 232 - Train Loss: 92.4529, Val Loss: 13.8393\n",
      "Epoch 233 - Train Loss: 92.3725, Val Loss: 13.8504\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 234 - Train Loss: 92.4012, Val Loss: 13.8576\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 235 - Train Loss: 92.3239, Val Loss: 13.8372\n",
      "Epoch 236 - Train Loss: 92.3755, Val Loss: 13.8482\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 237 - Train Loss: 92.2974, Val Loss: 13.8455\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 238 - Train Loss: 92.2469, Val Loss: 13.8304\n",
      "Epoch 239 - Train Loss: 92.2534, Val Loss: 13.8269\n",
      "Epoch 240 - Train Loss: 92.2860, Val Loss: 13.8360\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 241 - Train Loss: 92.2262, Val Loss: 13.8262\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 242 - Train Loss: 92.1569, Val Loss: 13.8273\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 243 - Train Loss: 92.1879, Val Loss: 13.8202\n",
      "Epoch 244 - Train Loss: 92.1684, Val Loss: 13.8359\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 245 - Train Loss: 92.1374, Val Loss: 13.8198\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 246 - Train Loss: 92.0950, Val Loss: 13.8223\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 247 - Train Loss: 92.1211, Val Loss: 13.8253\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 248 - Train Loss: 92.0555, Val Loss: 13.8185\n",
      "Epoch 249 - Train Loss: 92.0390, Val Loss: 13.8108\n",
      "Epoch 250 - Train Loss: 92.0278, Val Loss: 13.8027\n",
      "Epoch 251 - Train Loss: 92.0360, Val Loss: 13.8230\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 252 - Train Loss: 91.9499, Val Loss: 13.7898\n",
      "Epoch 253 - Train Loss: 91.9374, Val Loss: 13.8148\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 254 - Train Loss: 91.9078, Val Loss: 13.8270\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 255 - Train Loss: 91.8791, Val Loss: 13.7889\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 256 - Train Loss: 91.8847, Val Loss: 13.8040\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 257 - Train Loss: 91.8677, Val Loss: 13.8043\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 258 - Train Loss: 91.8684, Val Loss: 13.8005\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 259 - Train Loss: 91.8269, Val Loss: 13.8126\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 260 - Train Loss: 91.7664, Val Loss: 13.7986\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 261 - Train Loss: 91.8127, Val Loss: 13.8058\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 262 - Train Loss: 91.7884, Val Loss: 13.7981\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Final Test Loss: 13.7981\n"
     ]
    }
   ],
   "source": [
    "from attention import trainer, trainer_PCA_comp_brute_force, trainer_PCA_comp_2_model\n",
    "from attetion_sep_size_heads import trainer_multidomain_strategyB\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "def save_tensor_to_txt(tensor, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write tensor dimensions\n",
    "        dims = tensor.size()\n",
    "        f.write(\" \".join(map(str, dims)) + \"\\n\")\n",
    "\n",
    "        # Iterate over the first dimension (slices)\n",
    "        for i in range(dims[0]):\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"Slice {i + 1}\\n\")\n",
    "            for j in range(dims[1]):  # Iterate over the second dimension (rows)\n",
    "                row = tensor[i, j].tolist()\n",
    "                f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "cwd=cwd.replace(r\"\\CODE\\AttentionDCA_python\\src\",'')\n",
    "\n",
    "nb_bins_PCA=35\n",
    "H = 64\n",
    "d= 10\n",
    "n_epochs = 500 #could go more down\n",
    "#domain1_end = 63 #if your protein msa input family has a domain division, this is the zero index of the last aminoacid of the first domain\n",
    "domain1_end = 62\n",
    "filename = cwd + r'\\CODE\\DataAttentionDCA\\jdoms\\jdoms_bacteria_train2.fasta' #new lisa for energy couplings: HKRR174\n",
    "structfile = None\n",
    "trainer_type = 'std' # 'std' or 'std_with_masks' 'multidomain'\n",
    "family = 'jdoms'\n",
    "\n",
    "if trainer_type == 'std':\n",
    "    domain1_end = 0\n",
    "    H1 = H2 = 0\n",
    "\n",
    "\n",
    "if trainer_type == 'std' or trainer_type == 'std_with_masks':\n",
    "    \n",
    "    model = trainer_PCA_comp_brute_force(\n",
    "        n_epochs=n_epochs,\n",
    "        H=H,\n",
    "        d=d,\n",
    "        filename=filename,\n",
    "        structfile=structfile,\n",
    "        losstype='without_J',\n",
    "        index_last_domain1=domain1_end,  # this value is the 0-index include the domain 1, for HK-RR is 63 (so 64 long domain 1) \n",
    "        #it is set to zero if i dont want to divide any domain\n",
    "        H1 = H1,\n",
    "        H2 = H2,\n",
    "        max_gap_frac=0.8,\n",
    "        nb_bins_PCA=nb_bins_PCA\n",
    "    )\n",
    "\n",
    "    # Create results directory\n",
    "    simul_name = f'{H}_{d}_{family}_without_J_{n_epochs}_PCA_brute_force_{nb_bins_PCA}_bins'\n",
    "    results_dir = f'./results/{simul_name}'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Save model parameters\n",
    "    save_tensor_to_txt(model.Q.data, \"./results/\"+simul_name+\"/Q_tensor.txt\")\n",
    "    save_tensor_to_txt(model.K.data, \"./results/\"+simul_name+\"/K_tensor.txt\")\n",
    "    save_tensor_to_txt(model.V.data, \"./results/\"+simul_name+\"/V_tensor.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
