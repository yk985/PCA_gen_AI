{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7522ef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences read: 14502\n",
      "Sequences after filtering: 14502\n",
      "Sampling 100000 pairs out of 105146751 total pairs.\n",
      "Mean fraction of identical positions (sampled): 0.3726355555555556\n",
      "Computed theta: 0.3263242011855491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14502/14502 [00:09<00:00, 1550.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3265.70225762244\n",
      "65\n",
      "Using device: cpu\n",
      "Epoch 1 - Train Loss: 1848.7006, Val Loss: 1080.5076\n",
      "Epoch 2 - Train Loss: 1198.8265, Val Loss: 744.6877\n",
      "Epoch 3 - Train Loss: 867.5356, Val Loss: 552.3678\n",
      "Epoch 4 - Train Loss: 679.6841, Val Loss: 435.1602\n",
      "Epoch 5 - Train Loss: 564.8753, Val Loss: 360.0553\n",
      "Epoch 6 - Train Loss: 488.4474, Val Loss: 308.0758\n",
      "Epoch 7 - Train Loss: 434.3793, Val Loss: 270.3544\n",
      "Epoch 8 - Train Loss: 394.5053, Val Loss: 241.5661\n",
      "Epoch 9 - Train Loss: 363.4616, Val Loss: 218.6425\n",
      "Epoch 10 - Train Loss: 338.2263, Val Loss: 199.6638\n",
      "Epoch 11 - Train Loss: 317.0290, Val Loss: 183.5814\n",
      "Epoch 12 - Train Loss: 299.1725, Val Loss: 169.8817\n",
      "Epoch 13 - Train Loss: 283.8175, Val Loss: 157.9678\n",
      "Epoch 14 - Train Loss: 270.4633, Val Loss: 147.5471\n",
      "Epoch 15 - Train Loss: 258.5585, Val Loss: 138.3335\n",
      "Epoch 16 - Train Loss: 248.1344, Val Loss: 130.1066\n",
      "Epoch 17 - Train Loss: 238.6848, Val Loss: 122.6584\n",
      "Epoch 18 - Train Loss: 230.2501, Val Loss: 115.9514\n",
      "Epoch 19 - Train Loss: 222.4860, Val Loss: 109.8409\n",
      "Epoch 20 - Train Loss: 215.4793, Val Loss: 104.2794\n",
      "Epoch 21 - Train Loss: 209.0470, Val Loss: 99.1974\n",
      "Epoch 22 - Train Loss: 203.1519, Val Loss: 94.5275\n",
      "Epoch 23 - Train Loss: 197.7288, Val Loss: 90.2386\n",
      "Epoch 24 - Train Loss: 192.7382, Val Loss: 86.2547\n",
      "Epoch 25 - Train Loss: 188.0602, Val Loss: 82.5511\n",
      "Epoch 26 - Train Loss: 183.7285, Val Loss: 79.1385\n",
      "Epoch 27 - Train Loss: 179.7762, Val Loss: 75.9623\n",
      "Epoch 28 - Train Loss: 176.0041, Val Loss: 72.9699\n",
      "Epoch 29 - Train Loss: 172.4859, Val Loss: 70.1984\n",
      "Epoch 30 - Train Loss: 169.2004, Val Loss: 67.5766\n",
      "Epoch 31 - Train Loss: 166.2005, Val Loss: 65.1646\n",
      "Epoch 32 - Train Loss: 163.2650, Val Loss: 62.8702\n",
      "Epoch 33 - Train Loss: 160.4532, Val Loss: 60.7321\n",
      "Epoch 34 - Train Loss: 157.9149, Val Loss: 58.7084\n",
      "Epoch 35 - Train Loss: 155.5642, Val Loss: 56.8022\n",
      "Epoch 36 - Train Loss: 153.2306, Val Loss: 55.0028\n",
      "Epoch 37 - Train Loss: 151.1344, Val Loss: 53.3099\n",
      "Epoch 38 - Train Loss: 149.0272, Val Loss: 51.6966\n",
      "Epoch 39 - Train Loss: 147.0359, Val Loss: 50.1845\n",
      "Epoch 40 - Train Loss: 145.1752, Val Loss: 48.7406\n",
      "Epoch 41 - Train Loss: 143.4657, Val Loss: 47.3661\n",
      "Epoch 42 - Train Loss: 141.7597, Val Loss: 46.0809\n",
      "Epoch 43 - Train Loss: 140.1171, Val Loss: 44.8485\n",
      "Epoch 44 - Train Loss: 138.6055, Val Loss: 43.6612\n",
      "Epoch 45 - Train Loss: 137.0985, Val Loss: 42.5445\n",
      "Epoch 46 - Train Loss: 135.7749, Val Loss: 41.4756\n",
      "Epoch 47 - Train Loss: 134.4136, Val Loss: 40.4530\n",
      "Epoch 48 - Train Loss: 133.1378, Val Loss: 39.4819\n",
      "Epoch 49 - Train Loss: 131.9368, Val Loss: 38.5653\n",
      "Epoch 50 - Train Loss: 130.7861, Val Loss: 37.6648\n",
      "Epoch 51 - Train Loss: 129.5809, Val Loss: 36.8374\n",
      "Epoch 52 - Train Loss: 128.6022, Val Loss: 36.0160\n",
      "Epoch 53 - Train Loss: 127.4895, Val Loss: 35.2326\n",
      "Epoch 54 - Train Loss: 126.5713, Val Loss: 34.4878\n",
      "Epoch 55 - Train Loss: 125.5620, Val Loss: 33.7834\n",
      "Epoch 56 - Train Loss: 124.6251, Val Loss: 33.0878\n",
      "Epoch 57 - Train Loss: 123.7237, Val Loss: 32.4258\n",
      "Epoch 58 - Train Loss: 122.9036, Val Loss: 31.8099\n",
      "Epoch 59 - Train Loss: 122.0312, Val Loss: 31.1988\n",
      "Epoch 60 - Train Loss: 121.2438, Val Loss: 30.6221\n",
      "Epoch 61 - Train Loss: 120.4727, Val Loss: 30.0622\n",
      "Epoch 62 - Train Loss: 119.7580, Val Loss: 29.5217\n",
      "Epoch 63 - Train Loss: 119.0221, Val Loss: 29.0189\n",
      "Epoch 64 - Train Loss: 118.3721, Val Loss: 28.5134\n",
      "Epoch 65 - Train Loss: 117.6216, Val Loss: 28.0373\n",
      "Epoch 66 - Train Loss: 117.0164, Val Loss: 27.5705\n",
      "Epoch 67 - Train Loss: 116.3746, Val Loss: 27.1433\n",
      "Epoch 68 - Train Loss: 115.7158, Val Loss: 26.6987\n",
      "Epoch 69 - Train Loss: 115.2946, Val Loss: 26.3017\n",
      "Epoch 70 - Train Loss: 114.6122, Val Loss: 25.9053\n",
      "Epoch 71 - Train Loss: 114.0994, Val Loss: 25.5175\n",
      "Epoch 72 - Train Loss: 113.5908, Val Loss: 25.1488\n",
      "Epoch 73 - Train Loss: 112.9913, Val Loss: 24.7886\n",
      "Epoch 74 - Train Loss: 112.5194, Val Loss: 24.4449\n",
      "Epoch 75 - Train Loss: 111.9809, Val Loss: 24.1122\n",
      "Epoch 76 - Train Loss: 111.6533, Val Loss: 23.7994\n",
      "Epoch 77 - Train Loss: 111.1002, Val Loss: 23.4824\n",
      "Epoch 78 - Train Loss: 110.6782, Val Loss: 23.1817\n",
      "Epoch 79 - Train Loss: 110.2815, Val Loss: 22.8831\n",
      "Epoch 80 - Train Loss: 109.8856, Val Loss: 22.6159\n",
      "Epoch 81 - Train Loss: 109.3632, Val Loss: 22.3313\n",
      "Epoch 82 - Train Loss: 109.0768, Val Loss: 22.0839\n",
      "Epoch 83 - Train Loss: 108.5945, Val Loss: 21.8221\n",
      "Epoch 84 - Train Loss: 108.2815, Val Loss: 21.5735\n",
      "Epoch 85 - Train Loss: 107.8506, Val Loss: 21.3342\n",
      "Epoch 86 - Train Loss: 107.5286, Val Loss: 21.1134\n",
      "Epoch 87 - Train Loss: 107.1422, Val Loss: 20.8874\n",
      "Epoch 88 - Train Loss: 106.7533, Val Loss: 20.6819\n",
      "Epoch 89 - Train Loss: 106.5441, Val Loss: 20.4621\n",
      "Epoch 90 - Train Loss: 106.2333, Val Loss: 20.2753\n",
      "Epoch 91 - Train Loss: 105.9020, Val Loss: 20.0677\n",
      "Epoch 92 - Train Loss: 105.6150, Val Loss: 19.8685\n",
      "Epoch 93 - Train Loss: 105.2623, Val Loss: 19.6873\n",
      "Epoch 94 - Train Loss: 104.9733, Val Loss: 19.5155\n",
      "Epoch 95 - Train Loss: 104.6933, Val Loss: 19.3373\n",
      "Epoch 96 - Train Loss: 104.4224, Val Loss: 19.1853\n",
      "Epoch 97 - Train Loss: 104.1266, Val Loss: 19.0144\n",
      "Epoch 98 - Train Loss: 103.8704, Val Loss: 18.8622\n",
      "Epoch 99 - Train Loss: 103.6358, Val Loss: 18.7142\n",
      "Epoch 100 - Train Loss: 103.3178, Val Loss: 18.5691\n",
      "Epoch 101 - Train Loss: 103.1946, Val Loss: 18.4045\n",
      "Epoch 102 - Train Loss: 102.9363, Val Loss: 18.2858\n",
      "Epoch 103 - Train Loss: 102.6476, Val Loss: 18.1428\n",
      "Epoch 104 - Train Loss: 102.4730, Val Loss: 18.0090\n",
      "Epoch 105 - Train Loss: 102.2534, Val Loss: 17.8803\n",
      "Epoch 106 - Train Loss: 101.9599, Val Loss: 17.7666\n",
      "Epoch 107 - Train Loss: 101.7779, Val Loss: 17.6410\n",
      "Epoch 108 - Train Loss: 101.6509, Val Loss: 17.5175\n",
      "Epoch 109 - Train Loss: 101.3550, Val Loss: 17.4126\n",
      "Epoch 110 - Train Loss: 101.1495, Val Loss: 17.2998\n",
      "Epoch 111 - Train Loss: 101.0331, Val Loss: 17.2021\n",
      "Epoch 112 - Train Loss: 100.7355, Val Loss: 17.0870\n",
      "Epoch 113 - Train Loss: 100.6057, Val Loss: 16.9991\n",
      "Epoch 114 - Train Loss: 100.4130, Val Loss: 16.9025\n",
      "Epoch 115 - Train Loss: 100.2703, Val Loss: 16.8185\n",
      "Epoch 116 - Train Loss: 100.0956, Val Loss: 16.7249\n",
      "Epoch 117 - Train Loss: 99.9016, Val Loss: 16.6331\n",
      "Epoch 118 - Train Loss: 99.7077, Val Loss: 16.5475\n",
      "Epoch 119 - Train Loss: 99.5797, Val Loss: 16.4561\n",
      "Epoch 120 - Train Loss: 99.4548, Val Loss: 16.3828\n",
      "Epoch 121 - Train Loss: 99.2820, Val Loss: 16.3019\n",
      "Epoch 122 - Train Loss: 99.1805, Val Loss: 16.2196\n",
      "Epoch 123 - Train Loss: 98.9428, Val Loss: 16.1625\n",
      "Epoch 124 - Train Loss: 98.8201, Val Loss: 16.0745\n",
      "Epoch 125 - Train Loss: 98.7289, Val Loss: 16.0291\n",
      "Epoch 126 - Train Loss: 98.5752, Val Loss: 15.9404\n",
      "Epoch 127 - Train Loss: 98.3356, Val Loss: 15.8675\n",
      "Epoch 128 - Train Loss: 98.3342, Val Loss: 15.8196\n",
      "Epoch 129 - Train Loss: 98.1434, Val Loss: 15.7591\n",
      "Epoch 130 - Train Loss: 97.9578, Val Loss: 15.6930\n",
      "Epoch 131 - Train Loss: 97.8825, Val Loss: 15.6455\n",
      "Epoch 132 - Train Loss: 97.7217, Val Loss: 15.5780\n",
      "Epoch 133 - Train Loss: 97.6684, Val Loss: 15.5187\n",
      "Epoch 134 - Train Loss: 97.5454, Val Loss: 15.4842\n",
      "Epoch 135 - Train Loss: 97.4648, Val Loss: 15.4261\n",
      "Epoch 136 - Train Loss: 97.3479, Val Loss: 15.3688\n",
      "Epoch 137 - Train Loss: 97.2746, Val Loss: 15.3318\n",
      "Epoch 138 - Train Loss: 97.1645, Val Loss: 15.2788\n",
      "Epoch 139 - Train Loss: 97.0437, Val Loss: 15.2303\n",
      "Epoch 140 - Train Loss: 96.9870, Val Loss: 15.1898\n",
      "Epoch 141 - Train Loss: 96.8451, Val Loss: 15.1491\n",
      "Epoch 142 - Train Loss: 96.7107, Val Loss: 15.0951\n",
      "Epoch 143 - Train Loss: 96.6496, Val Loss: 15.0646\n",
      "Epoch 144 - Train Loss: 96.5852, Val Loss: 15.0288\n",
      "Epoch 145 - Train Loss: 96.4565, Val Loss: 14.9891\n",
      "Epoch 146 - Train Loss: 96.3321, Val Loss: 14.9558\n",
      "Epoch 147 - Train Loss: 96.2842, Val Loss: 14.9276\n",
      "Epoch 148 - Train Loss: 96.1854, Val Loss: 14.8849\n",
      "Epoch 149 - Train Loss: 96.0672, Val Loss: 14.8466\n",
      "Epoch 150 - Train Loss: 96.0275, Val Loss: 14.8176\n",
      "Epoch 151 - Train Loss: 95.9190, Val Loss: 14.7805\n",
      "Epoch 152 - Train Loss: 95.8051, Val Loss: 14.7580\n",
      "Epoch 153 - Train Loss: 95.7773, Val Loss: 14.7379\n",
      "Epoch 154 - Train Loss: 95.6573, Val Loss: 14.6917\n",
      "Epoch 155 - Train Loss: 95.5572, Val Loss: 14.6746\n",
      "Epoch 156 - Train Loss: 95.4901, Val Loss: 14.6441\n",
      "Epoch 157 - Train Loss: 95.4657, Val Loss: 14.6222\n",
      "Epoch 158 - Train Loss: 95.4368, Val Loss: 14.5926\n",
      "Epoch 159 - Train Loss: 95.3110, Val Loss: 14.5727\n",
      "Epoch 160 - Train Loss: 95.2799, Val Loss: 14.5442\n",
      "Epoch 161 - Train Loss: 95.1778, Val Loss: 14.5174\n",
      "Epoch 162 - Train Loss: 95.1479, Val Loss: 14.5090\n",
      "Epoch 163 - Train Loss: 95.0512, Val Loss: 14.4760\n",
      "Epoch 164 - Train Loss: 95.0151, Val Loss: 14.4715\n",
      "Epoch 165 - Train Loss: 94.9185, Val Loss: 14.4399\n",
      "Epoch 166 - Train Loss: 94.8653, Val Loss: 14.4233\n",
      "Epoch 167 - Train Loss: 94.7970, Val Loss: 14.4096\n",
      "Epoch 168 - Train Loss: 94.7648, Val Loss: 14.3751\n",
      "Epoch 169 - Train Loss: 94.7657, Val Loss: 14.3684\n",
      "Epoch 170 - Train Loss: 94.6459, Val Loss: 14.3502\n",
      "Epoch 171 - Train Loss: 94.6125, Val Loss: 14.3231\n",
      "Epoch 172 - Train Loss: 94.5632, Val Loss: 14.3161\n",
      "Epoch 173 - Train Loss: 94.4821, Val Loss: 14.2967\n",
      "Epoch 174 - Train Loss: 94.4073, Val Loss: 14.2886\n",
      "Epoch 175 - Train Loss: 94.4380, Val Loss: 14.2692\n",
      "Epoch 176 - Train Loss: 94.3540, Val Loss: 14.2668\n",
      "Epoch 177 - Train Loss: 94.2787, Val Loss: 14.2372\n",
      "Epoch 178 - Train Loss: 94.2248, Val Loss: 14.2361\n",
      "Epoch 179 - Train Loss: 94.2147, Val Loss: 14.2275\n",
      "Epoch 180 - Train Loss: 94.1747, Val Loss: 14.1912\n",
      "Epoch 181 - Train Loss: 94.0470, Val Loss: 14.2014\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 182 - Train Loss: 93.9938, Val Loss: 14.1627\n",
      "Epoch 183 - Train Loss: 94.0152, Val Loss: 14.1769\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 184 - Train Loss: 93.9535, Val Loss: 14.1352\n",
      "Epoch 185 - Train Loss: 93.9128, Val Loss: 14.1651\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 186 - Train Loss: 93.9142, Val Loss: 14.1298\n",
      "Epoch 187 - Train Loss: 93.8527, Val Loss: 14.1387\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 188 - Train Loss: 93.7793, Val Loss: 14.1071\n",
      "Epoch 189 - Train Loss: 93.7451, Val Loss: 14.1139\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 190 - Train Loss: 93.6967, Val Loss: 14.0884\n",
      "Epoch 191 - Train Loss: 93.6534, Val Loss: 14.0931\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 192 - Train Loss: 93.6364, Val Loss: 14.0799\n",
      "Epoch 193 - Train Loss: 93.6019, Val Loss: 14.0664\n",
      "Epoch 194 - Train Loss: 93.5470, Val Loss: 14.0688\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 195 - Train Loss: 93.5557, Val Loss: 14.0586\n",
      "Epoch 196 - Train Loss: 93.4598, Val Loss: 14.0475\n",
      "Epoch 197 - Train Loss: 93.3778, Val Loss: 14.0392\n",
      "Epoch 198 - Train Loss: 93.3346, Val Loss: 14.0292\n",
      "Epoch 199 - Train Loss: 93.3124, Val Loss: 14.0277\n",
      "Epoch 200 - Train Loss: 93.2857, Val Loss: 14.0260\n",
      "Epoch 201 - Train Loss: 93.2974, Val Loss: 14.0101\n",
      "Epoch 202 - Train Loss: 93.2392, Val Loss: 14.0182\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 203 - Train Loss: 93.1921, Val Loss: 13.9996\n",
      "Epoch 204 - Train Loss: 93.2141, Val Loss: 13.9897\n",
      "Epoch 205 - Train Loss: 93.1665, Val Loss: 13.9955\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 206 - Train Loss: 93.1622, Val Loss: 13.9968\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 207 - Train Loss: 93.0260, Val Loss: 13.9756\n",
      "Epoch 208 - Train Loss: 93.0098, Val Loss: 13.9793\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 209 - Train Loss: 93.0511, Val Loss: 13.9697\n",
      "Epoch 210 - Train Loss: 93.0041, Val Loss: 13.9558\n",
      "Epoch 211 - Train Loss: 92.9444, Val Loss: 13.9617\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 212 - Train Loss: 92.9510, Val Loss: 13.9651\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 213 - Train Loss: 92.8616, Val Loss: 13.9323\n",
      "Epoch 214 - Train Loss: 92.9147, Val Loss: 13.9473\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 215 - Train Loss: 92.8874, Val Loss: 13.9281\n",
      "Epoch 216 - Train Loss: 92.8666, Val Loss: 13.9416\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 217 - Train Loss: 92.8122, Val Loss: 13.9361\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 218 - Train Loss: 92.7907, Val Loss: 13.9212\n",
      "Epoch 219 - Train Loss: 92.7207, Val Loss: 13.9378\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 220 - Train Loss: 92.7369, Val Loss: 13.9047\n",
      "Epoch 221 - Train Loss: 92.6940, Val Loss: 13.9267\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 222 - Train Loss: 92.6543, Val Loss: 13.9037\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 223 - Train Loss: 92.6304, Val Loss: 13.9036\n",
      "Epoch 224 - Train Loss: 92.5996, Val Loss: 13.9117\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 225 - Train Loss: 92.5905, Val Loss: 13.8995\n",
      "Epoch 226 - Train Loss: 92.5878, Val Loss: 13.9035\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 227 - Train Loss: 92.5384, Val Loss: 13.9041\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 228 - Train Loss: 92.4964, Val Loss: 13.8929\n",
      "Epoch 229 - Train Loss: 92.4930, Val Loss: 13.8992\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 230 - Train Loss: 92.4836, Val Loss: 13.8908\n",
      "Epoch 231 - Train Loss: 92.3965, Val Loss: 13.8894\n",
      "Epoch 232 - Train Loss: 92.4316, Val Loss: 13.8757\n",
      "Epoch 233 - Train Loss: 92.3441, Val Loss: 13.8882\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 234 - Train Loss: 92.3649, Val Loss: 13.8869\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 235 - Train Loss: 92.2978, Val Loss: 13.8765\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 236 - Train Loss: 92.3641, Val Loss: 13.8809\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 237 - Train Loss: 92.2842, Val Loss: 13.8815\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 238 - Train Loss: 92.2279, Val Loss: 13.8764\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 239 - Train Loss: 92.2351, Val Loss: 13.8523\n",
      "Epoch 240 - Train Loss: 92.2648, Val Loss: 13.8733\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 241 - Train Loss: 92.2176, Val Loss: 13.8643\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 242 - Train Loss: 92.1424, Val Loss: 13.8597\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 243 - Train Loss: 92.1828, Val Loss: 13.8650\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 244 - Train Loss: 92.1529, Val Loss: 13.8657\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 245 - Train Loss: 92.1194, Val Loss: 13.8571\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 246 - Train Loss: 92.0841, Val Loss: 13.8652\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 247 - Train Loss: 92.1012, Val Loss: 13.8597\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 248 - Train Loss: 92.0503, Val Loss: 13.8607\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 249 - Train Loss: 92.0310, Val Loss: 13.8478\n",
      "Epoch 250 - Train Loss: 92.0251, Val Loss: 13.8442\n",
      "Epoch 251 - Train Loss: 92.0189, Val Loss: 13.8538\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 252 - Train Loss: 91.9446, Val Loss: 13.8343\n",
      "Epoch 253 - Train Loss: 91.9222, Val Loss: 13.8538\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 254 - Train Loss: 91.8944, Val Loss: 13.8593\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 255 - Train Loss: 91.8753, Val Loss: 13.8351\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 256 - Train Loss: 91.8651, Val Loss: 13.8474\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 257 - Train Loss: 91.8622, Val Loss: 13.8413\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 258 - Train Loss: 91.8619, Val Loss: 13.8355\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 259 - Train Loss: 91.8206, Val Loss: 13.8555\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 260 - Train Loss: 91.7760, Val Loss: 13.8381\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 261 - Train Loss: 91.8228, Val Loss: 13.8411\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 262 - Train Loss: 91.7967, Val Loss: 13.8345\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Final Test Loss: 13.8345\n"
     ]
    }
   ],
   "source": [
    "from attention import trainer, trainer_PCA_comp_brute_force, trainer_PCA_comp_2_model\n",
    "from attetion_sep_size_heads import trainer_multidomain_strategyB\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "def save_tensor_to_txt(tensor, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write tensor dimensions\n",
    "        dims = tensor.size()\n",
    "        f.write(\" \".join(map(str, dims)) + \"\\n\")\n",
    "\n",
    "        # Iterate over the first dimension (slices)\n",
    "        for i in range(dims[0]):\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"Slice {i + 1}\\n\")\n",
    "            for j in range(dims[1]):  # Iterate over the second dimension (rows)\n",
    "                row = tensor[i, j].tolist()\n",
    "                f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "cwd=cwd.replace(r\"\\CODE\\AttentionDCA_python\\src\",'')\n",
    "\n",
    "nb_bins_PCA=35\n",
    "H = 64\n",
    "d= 10\n",
    "n_epochs = 500 #could go more down\n",
    "#domain1_end = 63 #if your protein msa input family has a domain division, this is the zero index of the last aminoacid of the first domain\n",
    "domain1_end = 62\n",
    "filename = cwd + r'\\CODE\\DataAttentionDCA\\jdoms\\jdoms_bacteria_train2.fasta' #new lisa for energy couplings: HKRR174\n",
    "structfile = None\n",
    "trainer_type = 'std' # 'std' or 'std_with_masks' 'multidomain'\n",
    "family = 'jdoms'\n",
    "\n",
    "if trainer_type == 'std':\n",
    "    domain1_end = 0\n",
    "    H1 = H2 = 0\n",
    "\n",
    "\n",
    "if trainer_type == 'std' or trainer_type == 'std_with_masks':\n",
    "    \n",
    "    model = trainer_PCA_comp_brute_force(\n",
    "        n_epochs=n_epochs,\n",
    "        H=H,\n",
    "        d=d,\n",
    "        filename=filename,\n",
    "        structfile=structfile,\n",
    "        losstype='without_J',\n",
    "        index_last_domain1=domain1_end,  # this value is the 0-index include the domain 1, for HK-RR is 63 (so 64 long domain 1) \n",
    "        #it is set to zero if i dont want to divide any domain\n",
    "        H1 = H1,\n",
    "        H2 = H2,\n",
    "        max_gap_frac=0.8,\n",
    "        nb_bins_PCA=nb_bins_PCA\n",
    "    )\n",
    "\n",
    "    # Create results directory\n",
    "    simul_name = f'{H}_{d}_{family}_without_J_{n_epochs}_PCA_brute_force_{nb_bins_PCA}_bins'\n",
    "    results_dir = f'./results/{simul_name}'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Save model parameters\n",
    "    save_tensor_to_txt(model.Q.data, \"./results/\"+simul_name+\"/Q_tensor.txt\")\n",
    "    save_tensor_to_txt(model.K.data, \"./results/\"+simul_name+\"/K_tensor.txt\")\n",
    "    save_tensor_to_txt(model.V.data, \"./results/\"+simul_name+\"/V_tensor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16bde35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences read: 14502\n",
      "Sequences after filtering: 14502\n",
      "Sampling 100000 pairs out of 105146751 total pairs.\n",
      "Mean fraction of identical positions (sampled): 0.3723612698412698\n",
      "Computed theta: 0.32656457545070583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14502/14502 [00:09<00:00, 1527.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3265.70225762244\n",
      "Using device: cpu\n",
      "not working\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 1 - Train Loss: 1931.5890, Val Loss: 1431.8158\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 2 - Train Loss: 1069.2239, Val Loss: 879.7018\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 3 - Train Loss: 589.7693, Val Loss: 568.1368\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 4 - Train Loss: 306.5440, Val Loss: 376.2259\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 5 - Train Loss: 131.2471, Val Loss: 254.0687\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 6 - Train Loss: 21.3851, Val Loss: 180.6483\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 7 - Train Loss: -44.8808, Val Loss: 138.3380\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 8 - Train Loss: -81.1539, Val Loss: 118.7937\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 9 - Train Loss: -100.4944, Val Loss: 110.4331\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 10 - Train Loss: -110.8624, Val Loss: 108.3637\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 11 - Train Loss: -117.5620, Val Loss: 108.8969\n",
      "EarlyStopping counter: 1 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 12 - Train Loss: -121.9115, Val Loss: 110.5634\n",
      "EarlyStopping counter: 2 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 13 - Train Loss: -125.5305, Val Loss: 112.9630\n",
      "EarlyStopping counter: 3 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 14 - Train Loss: -128.6577, Val Loss: 116.5528\n",
      "EarlyStopping counter: 4 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 15 - Train Loss: -132.9080, Val Loss: 118.7515\n",
      "EarlyStopping counter: 5 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 16 - Train Loss: -137.0911, Val Loss: 121.2217\n",
      "EarlyStopping counter: 6 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 17 - Train Loss: -140.9790, Val Loss: 125.3858\n",
      "EarlyStopping counter: 7 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 18 - Train Loss: -144.5305, Val Loss: 128.6264\n",
      "EarlyStopping counter: 8 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 19 - Train Loss: -148.8242, Val Loss: 131.5604\n",
      "EarlyStopping counter: 9 out of 10\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Epoch 20 - Train Loss: -152.3162, Val Loss: 133.7664\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "N2:  2\n",
      "N2:  2\n",
      "N2:  2\n",
      "Final Test Loss: 133.7664\n"
     ]
    }
   ],
   "source": [
    "from attention import trainer, trainer_PCA_comp_brute_force, trainer_PCA_comp_2_model\n",
    "from attetion_sep_size_heads import trainer_multidomain_strategyB\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "def save_tensor_to_txt(tensor, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write tensor dimensions\n",
    "        dims = tensor.size()\n",
    "        f.write(\" \".join(map(str, dims)) + \"\\n\")\n",
    "\n",
    "        # Iterate over the first dimension (slices)\n",
    "        for i in range(dims[0]):\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"Slice {i + 1}\\n\")\n",
    "            for j in range(dims[1]):  # Iterate over the second dimension (rows)\n",
    "                row = tensor[i, j].tolist()\n",
    "                f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "cwd=cwd.replace(r\"\\CODE\\AttentionDCA_python\\src\",'')\n",
    "\n",
    "nb_bins_PCA=35\n",
    "H = 64\n",
    "d= 10\n",
    "n_epochs = 500 #could go more down\n",
    "#domain1_end = 63 #if your protein msa input family has a domain division, this is the zero index of the last aminoacid of the first domain\n",
    "domain1_end = 62\n",
    "filename = cwd + r'\\CODE\\DataAttentionDCA\\jdoms\\jdoms_bacteria_train2.fasta' #new lisa for energy couplings: HKRR174\n",
    "structfile = None\n",
    "trainer_type = 'std' # 'std' or 'std_with_masks' 'multidomain'\n",
    "family = 'jdoms'\n",
    "\n",
    "if trainer_type == 'std':\n",
    "    domain1_end = 0\n",
    "    H1 = H2 = 0\n",
    "\n",
    "\n",
    "if trainer_type == 'std' or trainer_type == 'std_with_masks':\n",
    "    \n",
    "    model = trainer_PCA_comp_2_model(\n",
    "        n_epochs=n_epochs,\n",
    "        H=H,\n",
    "        d=d,\n",
    "        filename=filename,\n",
    "        structfile=structfile,\n",
    "        losstype='without_J',\n",
    "        index_last_domain1=domain1_end,  # this value is the 0-index include the domain 1, for HK-RR is 63 (so 64 long domain 1) \n",
    "        #it is set to zero if i dont want to divide any domain\n",
    "        H1 = H1,\n",
    "        H2 = H2,\n",
    "        max_gap_frac=0.8,\n",
    "        nb_bins_PCA=nb_bins_PCA\n",
    "    )\n",
    "\n",
    "    # Create results directory\n",
    "    simul_name = f'{H}_{d}_{family}_without_J_{n_epochs}_PCA_2models_{nb_bins_PCA}_bins'\n",
    "    results_dir = f'./results/{simul_name}'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Save model parameters\n",
    "    save_tensor_to_txt(model.Q.data, \"./results/\"+simul_name+\"/Q_tensor.txt\")\n",
    "    save_tensor_to_txt(model.K.data, \"./results/\"+simul_name+\"/K_tensor.txt\")\n",
    "    save_tensor_to_txt(model.V.data, \"./results/\"+simul_name+\"/V_tensor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16df0851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences read: 14502\n",
      "Sequences after filtering: 14502\n",
      "Sampling 100000 pairs out of 105146751 total pairs.\n",
      "Mean fraction of identical positions (sampled): 0.37189095300163066\n",
      "Computed theta: 0.3269775696841617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14502/14502 [00:09<00:00, 1486.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3265.70225762244\n",
      "Using device: cpu\n",
      "not working\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 1 - Train Loss: 73761.2992, Val Loss: 45958.6198\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 2 - Train Loss: 32030.4849, Val Loss: 19482.5560\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 3 - Train Loss: 13514.9996, Val Loss: 8199.4368\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 4 - Train Loss: 5691.9171, Val Loss: 3460.0531\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 5 - Train Loss: 2404.0533, Val Loss: 1468.2622\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 6 - Train Loss: 1021.2553, Val Loss: 629.0251\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 7 - Train Loss: 437.1342, Val Loss: 273.7248\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 8 - Train Loss: 189.1023, Val Loss: 122.4298\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 9 - Train Loss: 82.8411, Val Loss: 57.2596\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 10 - Train Loss: 36.9711, Val Loss: 28.8871\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 11 - Train Loss: 16.9749, Val Loss: 16.5071\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 12 - Train Loss: 8.1317, Val Loss: 11.0198\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 13 - Train Loss: 4.3552, Val Loss: 8.6564\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 14 - Train Loss: 2.6999, Val Loss: 7.9274\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 15 - Train Loss: 1.9129, Val Loss: 7.2382\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 16 - Train Loss: 1.6607, Val Loss: 7.0941\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 17 - Train Loss: 1.5492, Val Loss: 7.1907\n",
      "EarlyStopping counter: 1 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 18 - Train Loss: 1.5514, Val Loss: 7.1220\n",
      "EarlyStopping counter: 2 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 19 - Train Loss: 1.5481, Val Loss: 7.1860\n",
      "EarlyStopping counter: 3 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 20 - Train Loss: 1.6353, Val Loss: 6.8991\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 21 - Train Loss: 1.5558, Val Loss: 7.0887\n",
      "EarlyStopping counter: 1 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 22 - Train Loss: 1.6305, Val Loss: 7.0951\n",
      "EarlyStopping counter: 2 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 23 - Train Loss: 1.5083, Val Loss: 7.0672\n",
      "EarlyStopping counter: 3 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 24 - Train Loss: 1.5826, Val Loss: 7.0787\n",
      "EarlyStopping counter: 4 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 25 - Train Loss: 1.5784, Val Loss: 7.1624\n",
      "EarlyStopping counter: 5 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 26 - Train Loss: 1.6193, Val Loss: 7.2672\n",
      "EarlyStopping counter: 6 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 27 - Train Loss: 1.6422, Val Loss: 7.1227\n",
      "EarlyStopping counter: 7 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 28 - Train Loss: 1.6212, Val Loss: 7.0721\n",
      "EarlyStopping counter: 8 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 29 - Train Loss: 1.6345, Val Loss: 7.2498\n",
      "EarlyStopping counter: 9 out of 10\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Epoch 30 - Train Loss: 1.7660, Val Loss: 7.1300\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "N2:  1\n",
      "N2:  1\n",
      "N2:  1\n",
      "Final Test Loss: 7.1300\n"
     ]
    }
   ],
   "source": [
    "from attention import trainer, trainer_PCA_comp_2_model_flat\n",
    "from attetion_sep_size_heads import trainer_multidomain_strategyB\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "def save_tensor_to_txt(tensor, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        # Write tensor dimensions\n",
    "        dims = tensor.size()\n",
    "        f.write(\" \".join(map(str, dims)) + \"\\n\")\n",
    "\n",
    "        # Iterate over the first dimension (slices)\n",
    "        for i in range(dims[0]):\n",
    "            f.write(\"\\n\")\n",
    "            f.write(f\"Slice {i + 1}\\n\")\n",
    "            for j in range(dims[1]):  # Iterate over the second dimension (rows)\n",
    "                row = tensor[i, j].tolist()\n",
    "                f.write(\",\".join(map(str, row)) + \"\\n\")\n",
    "cwd=cwd.replace(\"/CODE/AttentionDCA_python/src\",'')\n",
    "\n",
    "nb_bins_PCA=35\n",
    "H = 64\n",
    "d= 10\n",
    "n_epochs = 500 #could go more down\n",
    "#domain1_end = 63 #if your protein msa input family has a domain division, this is the zero index of the last aminoacid of the first domain\n",
    "domain1_end = 62\n",
    "filename = cwd + '/CODE/DataAttentionDCA/jdoms/jdoms_bacteria_train2.fasta' #new lisa for energy couplings: HKRR174\n",
    "structfile = None\n",
    "trainer_type = 'std' # 'std' or 'std_with_masks' 'multidomain'\n",
    "family = 'jdoms'\n",
    "\n",
    "if trainer_type == 'std':\n",
    "    domain1_end = 0\n",
    "    H1 = H2 = 0\n",
    "\n",
    "\n",
    "if trainer_type == 'std' or trainer_type == 'std_with_masks':\n",
    "    \n",
    "    model = trainer_PCA_comp_2_model_flat(\n",
    "        n_epochs=n_epochs,\n",
    "        H=H,\n",
    "        d=d,\n",
    "        filename=filename,\n",
    "        structfile=structfile,\n",
    "        losstype='without_J',\n",
    "        index_last_domain1=domain1_end,  # this value is the 0-index include the domain 1, for HK-RR is 63 (so 64 long domain 1) \n",
    "        #it is set to zero if i dont want to divide any domain\n",
    "        H1 = H1,\n",
    "        H2 = H2,\n",
    "        max_gap_frac=0.8,\n",
    "        nb_bins_PCA=nb_bins_PCA\n",
    "    )\n",
    "\n",
    "    # Create results directory\n",
    "    simul_name = f'{H}_{d}_{family}_without_J_{n_epochs}_PCA_2models_flat_{nb_bins_PCA}_bins'\n",
    "    results_dir = f'./results/{simul_name}'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Save model parameters\n",
    "    save_tensor_to_txt(model.Q.data, \"./results/\"+simul_name+\"/Q_tensor.txt\")\n",
    "    save_tensor_to_txt(model.K.data, \"./results/\"+simul_name+\"/K_tensor.txt\")\n",
    "    save_tensor_to_txt(model.V.data, \"./results/\"+simul_name+\"/V_tensor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
