{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44eb2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from model import AttentionModel\n",
    "from dcascore import *\n",
    "# back to original path (in PLM)\n",
    "sys.path.pop(0)  # Removes the parent_dir from sys.path\n",
    "from model import AttentionModel\n",
    "\n",
    "from plm_gen_methods import generate_plm_n_save\n",
    "from seq_utils import read_tensor_from_txt, set_seed, letters_to_nums, modify_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9b89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\youss\\OneDrive\\Bureau\\master epfl\\MA2\\TP4 De los Rios\\PCA_gen_ai\\PCA_gen_AI\\CODE\\AttentionDCA_python\\src\\PLM\n",
      "c:\\Users\\youss\\OneDrive\\Bureau\\master epfl\\MA2\\TP4 De los Rios\\PCA_gen_ai\\PCA_gen_AI\\CODE\\AttentionDCA_python\\src\n"
     ]
    }
   ],
   "source": [
    "print(current_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e86555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Load Q, K, V matrices from jdoms (after training)\n",
    "\"\"\"\n",
    "set_seed()\n",
    "H = 64\n",
    "d= 10\n",
    "N = 174\n",
    "n_epochs = 500\n",
    "loss_type = 'without_J'\n",
    "family = 'jdoms' #'jdoms_bacteria_train2'\n",
    "cwd = parent_dir\n",
    "Q_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_PCA_brute_force_35_bins/Q_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "K_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_PCA_brute_force_35_bins/K_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "V_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_PCA_brute_force_35_bins/V_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "H,d,N=Q_1.shape\n",
    "q=V_1.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d8be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 65, 65])\n",
      "34\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "model=AttentionModel(H,d,N,q,Q=Q_1,V=V_1,K=K_1)\n",
    "torch.sum(model.Q-Q_1)\n",
    "device = Q_1.device\n",
    "L = Q_1.shape[-1]\n",
    "W=attention_heads_from_model(model,Q_1,K_1,V_1)\n",
    "print(W.shape)\n",
    "\n",
    "i_indices = torch.arange(L, device=device).unsqueeze(1)\n",
    "j_indices = torch.arange(L, device=device).unsqueeze(0)\n",
    "mask = (i_indices != j_indices).float().unsqueeze(0)  # shape (1, L, L)\n",
    "W = W * mask\n",
    "    \n",
    "# Compute Jtens\n",
    "Jtens = torch.einsum('hri,hab->abri', W, V_1)  # Shape: (q, q, L, L)\n",
    "q = Jtens.shape[0]\n",
    "N = Jtens.shape[2]\n",
    "print(q)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465dd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generate sequences with PLM random initialization\n",
    "\"\"\"\n",
    "save_dir = \"generated_sequences\"\n",
    "N_seqs = 30000\n",
    "save_name = \"generated_sequences_randinit_30000\"\n",
    "generate_plm_n_save(save_dir, save_name, Jtens, N_seqs=40000, init_sequence=None,nb_PCA_comp=2,PCA_comp_list=np.array([20,20]))\n",
    "\n",
    "##############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generate sequences with PLM initialization from a sequence\n",
    "\"\"\"\n",
    "init_sequence = 'DYYQVLGVPKDADAKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANEANEVLSDPEKRKKYD'\n",
    "init_sequence_num = letters_to_nums(init_sequence)\n",
    "ratio = 0.1\n",
    "init_sequence_num = modify_seq(init_sequence_num, ratio)\n",
    "N_seqs=40000\n",
    "save_name = f\"gen_seqs_w_init_seq_Ns{N_seqs}_r{ratio}\"\n",
    "\n",
    "#generate_plm_n_save(save_dir, save_name, Jtens, N_seqs, init_sequence=init_sequence_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Generate sequences with PLM initialization from a sequence different betas\n",
    "\"\"\"\n",
    "init_sequence = 'DYYQVLGVPKDADAKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANEANEVLSDPEKRKKYD'\n",
    "init_sequence_num = letters_to_nums(init_sequence)\n",
    "ratio = 0.1\n",
    "init_sequence_num = modify_seq(init_sequence_num, ratio)\n",
    "\n",
    "N_seqs=4000\n",
    "betas = [0.01, 0.1, 0.5, 1, 2, 4, 10]\n",
    "for b in betas:\n",
    "    save_name = f\"gen_seqs_w_init_seq_Ns{N_seqs}_r{ratio}_b{b}\"\n",
    "\n",
    "    generate_plm_n_save(save_dir, save_name, Jtens, N_seqs, init_sequence=init_sequence_num, beta=b)\n",
    "    #generate_plm_n_save(save_dir, save_name, Jtens, N_seqs, init_sequence=init_sequence_num, beta=b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
