{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Gen methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'A', 20: '-', 1: 'C', 2: 'D', 3: 'E', 4: 'F', 5: 'G', 6: 'H', 7: 'I', 8: 'K', 9: 'L', 10: 'M', 11: 'N', 12: 'P', 13: 'Q', 14: 'R', 15: 'S', 16: 'T', 17: 'V', 18: 'W', 19: 'Y'}\n"
     ]
    }
   ],
   "source": [
    "from plm_model import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "#current_dir = os.path.dirname(__file__)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from model import AttentionModel\n",
    "from attention import trainer\n",
    "from dcascore import *\n",
    "# back to original path (in PLM)\n",
    "sys.path.pop(0)  # Removes the parent_dir from sys.path\n",
    "from plm_methods import read_tensor_from_txt, set_seed, letters_to_nums, modify_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Q, K, V; compute J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 63, 63])\n",
      "21\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "\"\"\"\n",
    "    Load Q, K, V matrices from jdoms (after training)\n",
    "\"\"\"\n",
    "set_seed()\n",
    "H = 64\n",
    "d= 10\n",
    "n_epochs = 500\n",
    "loss_type = 'without_J'\n",
    "family = 'jdoms' #'jdoms_bacteria_train2'\n",
    "#cwd = '/Users/marzioformica/Desktop/EPFL/Master/MA2/Labo/my_project/PLM-gen-DCA/Attention-DCA-main/CODE/AttentionDCA_python/src'\n",
    "cwd='C:\\Users\\youss\\OneDrive\\Bureau\\master epfl\\MA2\\TP4 De los Rios\\git_test\\PLM-gen-DCA\\Attention-DCA-main\\CODE\\AttentionDCA_python\\src'\n",
    "\n",
    "# Q_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/Q_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "# K_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/K_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "# V_1 = read_tensor_from_txt( cwd +\"/results/{H}_{d}_{family}_{losstype}_{n_epochs}_youss/V_tensor.txt\".format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "Q_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\Q_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "K_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\K_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "V_1 = read_tensor_from_txt( cwd +r'\\results\\{H}_{d}_{family}_{losstype}_{n_epochs}_youss\\V_tensor.txt'.format(H=H, d=d, family=family, losstype=loss_type, n_epochs=n_epochs))\n",
    "H,d,N=Q_1.shape\n",
    "q=V_1.shape[1]\n",
    "\n",
    "##############################################################\n",
    "\"\"\"\n",
    "    Initialize the model and compute couplings J from Q, K, V\n",
    "\"\"\" \n",
    "model=AttentionModel(H,d,N,q,Q=Q_1,V=V_1,K=K_1)\n",
    "torch.sum(model.Q-Q_1)\n",
    "device = Q_1.device\n",
    "L = Q_1.shape[-1]\n",
    "W=attention_heads_from_model(model,Q_1,K_1,V_1)\n",
    "print(W.shape)\n",
    "\n",
    "i_indices = torch.arange(L, device=device).unsqueeze(1)\n",
    "j_indices = torch.arange(L, device=device).unsqueeze(0)\n",
    "mask = (i_indices != j_indices).float().unsqueeze(0)  # shape (1, L, L)\n",
    "W = W * mask\n",
    "    \n",
    "# Compute Jtens\n",
    "Jtens = torch.einsum('hri,hab->abri', W, V_1)  # Shape: (q, q, L, L)\n",
    "q = Jtens.shape[0]\n",
    "N = Jtens.shape[2]\n",
    "print(q)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.4190e-02, -1.1062e-03, -7.9344e-02, -6.0122e-03,  1.8884e-02,\n",
      "          3.7376e-02, -6.7353e-03, -2.6382e-02,  1.9143e-02,  9.7130e-02,\n",
      "         -4.3858e-02, -3.0229e-02,  4.0515e-02, -1.2474e-02,  5.2964e-02,\n",
      "         -6.5531e-02,  6.6707e-03,  2.4711e-02, -1.1974e-02, -1.7580e-02,\n",
      "          2.6321e-02],\n",
      "        [-1.9488e-02, -2.1032e-04, -2.5435e-02,  5.5132e-03, -1.3635e-02,\n",
      "         -8.1761e-03,  3.0495e-03, -1.3606e-02, -1.4065e-02, -4.8213e-02,\n",
      "         -4.1118e-03, -8.1658e-03,  4.1129e-03,  1.3696e-03, -1.2804e-02,\n",
      "         -2.0105e-02, -7.4580e-03, -1.5246e-02, -4.5735e-03,  1.1006e-02,\n",
      "         -2.0161e-03],\n",
      "        [-3.4951e-02,  5.6149e-03, -9.6376e-02, -8.7806e-02,  4.6556e-02,\n",
      "         -1.1157e-02, -9.2960e-03, -7.7389e-04,  1.4078e-01, -4.5730e-02,\n",
      "         -2.6454e-03,  1.3194e-02, -1.9140e-02,  2.7268e-02,  1.5225e-01,\n",
      "          5.0871e-02, -1.6868e-02, -1.0864e-02, -2.9854e-03, -8.3227e-02,\n",
      "          1.3813e-04],\n",
      "        [-2.4088e-02,  1.1599e-02, -3.6280e-02, -7.9341e-02,  9.8437e-03,\n",
      "          1.0385e-02,  1.1356e-02, -1.9478e-02,  3.2047e-02,  9.6128e-02,\n",
      "          4.0996e-02,  1.2873e-02,  2.4368e-02, -2.0821e-03,  2.2109e-02,\n",
      "         -1.0144e-03,  2.2620e-02,  3.6586e-02, -5.4336e-03, -6.8156e-02,\n",
      "         -1.8516e-02],\n",
      "        [ 9.1203e-03,  6.2104e-04,  3.3149e-02, -4.1672e-02, -3.2287e-02,\n",
      "         -2.0719e-02, -1.0228e-02,  3.6442e-02, -3.7395e-04, -4.5900e-02,\n",
      "          1.2231e-02, -2.3171e-02, -2.1866e-02,  2.3010e-02, -2.6512e-02,\n",
      "          1.5387e-02, -3.1407e-03, -3.9758e-03, -1.0360e-02,  1.1509e-01,\n",
      "         -1.4912e-02],\n",
      "        [ 3.1002e-02, -3.2537e-03,  7.8849e-03,  8.5773e-03, -8.6456e-03,\n",
      "          1.4984e-02, -6.1368e-03, -3.2114e-02,  3.9072e-02,  1.5739e-02,\n",
      "          3.6508e-03,  1.4015e-02,  1.2989e-02, -2.7050e-02,  3.2951e-02,\n",
      "         -2.5658e-02, -2.0344e-02, -9.2869e-03,  4.4105e-03, -4.5384e-02,\n",
      "         -3.5110e-02],\n",
      "        [ 5.1113e-02,  3.9601e-03, -4.0990e-02, -2.6338e-03, -1.4122e-02,\n",
      "         -1.1692e-02,  1.0625e-02, -1.6433e-02,  2.2454e-02, -3.8601e-02,\n",
      "         -1.1071e-02, -1.3801e-02, -1.0600e-03,  9.1481e-03,  2.7030e-02,\n",
      "          1.1482e-03, -2.4454e-03,  4.9786e-03,  1.6397e-03,  2.5331e-02,\n",
      "         -2.9847e-02],\n",
      "        [-5.8505e-02,  5.8002e-04,  3.5066e-02, -4.6481e-02,  2.6544e-02,\n",
      "          7.7515e-03, -2.9849e-02,  1.5059e-03,  2.9371e-02, -5.0438e-02,\n",
      "          4.8334e-04,  2.0698e-02, -1.2417e-02,  1.0048e-02, -2.0128e-02,\n",
      "          9.2007e-03,  8.7720e-03,  4.2429e-02, -3.2706e-03,  5.0861e-02,\n",
      "         -7.3443e-03],\n",
      "        [-2.8184e-02,  1.5072e-03,  1.7551e-01,  1.0745e-01,  3.9956e-02,\n",
      "          3.1298e-02, -2.9780e-02,  5.8586e-02, -1.1213e-01,  5.9015e-03,\n",
      "         -1.1025e-02, -1.4308e-02, -4.7219e-02,  3.3147e-04, -1.5454e-01,\n",
      "          1.9739e-02,  2.2043e-02,  1.1766e-02,  1.0861e-03, -1.4559e-02,\n",
      "         -1.2898e-02],\n",
      "        [ 5.5565e-02, -3.1872e-03, -3.9533e-02, -1.1344e-02, -4.2077e-03,\n",
      "         -4.0435e-02, -7.7700e-04,  8.2686e-02, -4.4683e-02, -3.0314e-03,\n",
      "         -3.9358e-04, -1.6376e-02,  4.9142e-02, -6.0099e-02, -5.3792e-02,\n",
      "          3.6498e-02,  8.4935e-02,  1.1263e-02,  1.4912e-02,  5.1406e-02,\n",
      "          1.7852e-03],\n",
      "        [-3.5784e-02, -4.8083e-03, -1.5569e-02, -1.9071e-02, -4.1534e-03,\n",
      "         -4.7680e-03,  2.0660e-02, -6.4731e-03,  3.8576e-02, -2.3442e-02,\n",
      "          8.1054e-03,  5.2581e-03,  1.5503e-02, -4.6971e-02, -2.3024e-02,\n",
      "         -6.9359e-03, -3.4030e-02, -1.3600e-02,  4.0971e-03,  4.7773e-02,\n",
      "          1.6792e-02],\n",
      "        [-3.8492e-02, -2.0892e-03,  4.8296e-02,  4.4568e-02,  6.4779e-03,\n",
      "         -6.1907e-03,  1.5880e-02, -2.8522e-02, -1.8922e-02, -3.3699e-02,\n",
      "          1.2938e-02,  6.9013e-02, -4.8462e-02,  3.6773e-02, -2.2876e-02,\n",
      "          8.3707e-02, -9.6730e-03, -3.9273e-02,  3.1679e-03, -3.9328e-02,\n",
      "         -1.3793e-02],\n",
      "        [ 5.5673e-02,  2.2076e-03, -5.3357e-02,  1.5621e-02, -1.0264e-02,\n",
      "         -3.0936e-02, -3.0151e-02,  1.3146e-03,  3.7711e-04, -4.9010e-02,\n",
      "         -6.4508e-03, -1.2004e-02,  4.8647e-02,  7.4164e-03, -1.5929e-03,\n",
      "          3.4840e-03,  3.8420e-03, -1.4448e-02, -4.8744e-03, -5.6557e-02,\n",
      "          6.7725e-02],\n",
      "        [ 8.4054e-03, -5.5401e-03, -3.8468e-02, -2.3033e-02, -6.4255e-03,\n",
      "          4.1971e-03,  1.1531e-03,  5.8766e-03,  2.9684e-02, -3.4149e-02,\n",
      "          2.2087e-02,  1.4882e-02, -3.6444e-03,  5.1630e-02, -1.4573e-02,\n",
      "          2.0137e-02,  1.5486e-02,  2.5557e-03, -9.5824e-03,  1.6116e-02,\n",
      "         -5.8160e-03],\n",
      "        [ 2.4189e-02,  1.1868e-04,  1.5364e-01,  1.1574e-01,  1.3778e-02,\n",
      "          8.0113e-03,  3.8417e-02,  3.9500e-02, -1.2636e-01,  4.8434e-02,\n",
      "          6.9468e-04, -1.4720e-02, -5.2312e-02, -2.8370e-02,  1.1824e-02,\n",
      "         -6.9099e-02, -3.1015e-02, -1.3749e-02, -3.4797e-03, -4.4475e-02,\n",
      "         -1.6478e-03],\n",
      "        [-5.0225e-02, -3.4243e-03,  1.6074e-02,  2.2343e-02, -2.4260e-03,\n",
      "         -1.7164e-02,  8.9050e-03, -2.8440e-02,  7.2118e-03,  6.2994e-02,\n",
      "         -7.0328e-03,  3.8751e-02,  1.1624e-02,  3.2088e-02, -3.5489e-02,\n",
      "         -3.2126e-03, -1.6722e-02,  2.0851e-02, -2.7981e-03, -2.4435e-03,\n",
      "          2.1988e-02],\n",
      "        [ 3.0201e-02, -1.2009e-02,  4.4420e-02,  3.4824e-02, -3.0902e-02,\n",
      "          1.6397e-02,  1.6855e-02, -1.4448e-02, -2.6184e-02,  7.2061e-02,\n",
      "         -1.4752e-02, -2.2138e-02,  1.1013e-03, -1.3166e-02, -5.0248e-02,\n",
      "         -1.6443e-02,  3.8188e-03, -1.1205e-02,  5.0948e-03, -3.2884e-02,\n",
      "          2.5410e-02],\n",
      "        [-1.0011e-02,  1.1650e-02, -4.7561e-02,  8.4342e-03, -1.7592e-02,\n",
      "         -7.9422e-03, -1.5421e-02, -2.5503e-03, -6.8626e-03,  1.3835e-02,\n",
      "          3.2784e-03, -8.7170e-03,  4.0875e-02, -2.2404e-03,  7.0275e-02,\n",
      "         -1.9781e-02, -1.9367e-02,  8.9156e-03,  3.2628e-03,  3.0390e-03,\n",
      "          2.4790e-02],\n",
      "        [-3.6604e-02, -1.4876e-03, -4.2324e-02,  8.5947e-04, -1.1808e-02,\n",
      "          3.2972e-02, -1.7695e-03, -1.7280e-02, -1.0629e-02, -4.2507e-02,\n",
      "         -5.4981e-03, -5.9454e-03, -2.6130e-02,  1.2104e-02,  5.0048e-02,\n",
      "         -2.8331e-02, -2.9902e-02, -1.7950e-02,  2.5545e-02,  8.9363e-03,\n",
      "         -1.0387e-02],\n",
      "        [-4.0293e-02,  1.6644e-03, -4.6617e-02, -1.7416e-02,  1.5160e-02,\n",
      "         -1.0825e-02, -3.1332e-03, -2.0634e-02,  3.0226e-02,  1.5160e-02,\n",
      "         -3.7977e-03, -2.9849e-02, -3.3316e-03, -1.4425e-02,  1.5610e-02,\n",
      "          1.7623e-02,  2.4698e-02, -1.7334e-02, -2.6931e-03,  6.3032e-02,\n",
      "          1.4701e-02],\n",
      "        [ 9.8539e-03, -3.2270e-03,  2.6350e-02, -1.9195e-02, -1.6360e-02,\n",
      "          1.0025e-02,  1.7844e-02,  7.8193e-03, -4.1829e-02, -4.8901e-03,\n",
      "          3.5858e-03,  8.0121e-03, -1.7939e-02,  3.1159e-03, -1.3444e-02,\n",
      "         -6.4240e-04, -8.3531e-04,  6.8614e-03, -1.7205e-03,  6.8924e-03,\n",
      "         -5.0493e-02]])\n"
     ]
    }
   ],
   "source": [
    "print(Jtens[:,:,30,31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def read_tensor_from_txt(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Read the dimensions from the first line\n",
    "    dims = list(map(int, lines[0].strip().split()))\n",
    "    \n",
    "    tensor_data = []\n",
    "    current_slice = []\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Slice\"):\n",
    "            if current_slice:\n",
    "                tensor_data.append(current_slice)\n",
    "                current_slice = []\n",
    "        elif line:\n",
    "            current_slice.append(list(map(float, line.split(','))))\n",
    "    if current_slice:\n",
    "        tensor_data.append(current_slice)\n",
    "\n",
    "    tensor = torch.tensor(tensor_data).view(*dims)\n",
    "    return tensor\n",
    "def set_seed(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "def modify_seq(seq,ratio,aa_list=np.arange(21)):\n",
    "    L=len(seq)\n",
    "    seq_func=seq.copy()\n",
    "    nb_change=int(L*ratio)\n",
    "    ind_change=np.random.choice(np.arange(L),nb_change)\n",
    "    for i in range(nb_change):\n",
    "        seq_func[ind_change[i]]=np.random.choice(aa_list)\n",
    "    return seq_func\n",
    "def invert_dict(d):\n",
    "    inverted = {}\n",
    "    for key, value in d.items():\n",
    "        if value in inverted:\n",
    "            inverted[value].append(key)\n",
    "        else:\n",
    "            inverted[value] = [key]\n",
    "    return inverted\n",
    "def numbers_to_letters(numbers, inverted_dict):\n",
    "    result = ''\n",
    "    for number in numbers:\n",
    "        if number in inverted_dict:\n",
    "            letters = inverted_dict[number]\n",
    "            result += random.choice(letters)\n",
    "        else:\n",
    "            raise ValueError(f\"Number {number} not found in the inverted dictionary.\")\n",
    "    return result\n",
    "def seq_num_to_letters(file_path,dictionary):\n",
    "    inv_dict=invert_dict(dictionary)\n",
    "    seq_array=np.loadtxt(file_path).astype(np.int64)\n",
    "    letter_seq_array=[]\n",
    "    for seq in seq_array:\n",
    "        letter_seq_array.append(numbers_to_letters(seq,inv_dict))\n",
    "    output_path=file_path.replace(\".txt\",'')+\"output.txt\"\n",
    "    np.savetxt(output_path, np.array(letter_seq_array), fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_num_ale = {\n",
    "    'A': 1,  'B': 21, 'C': 2,  'D': 3,  'E': 4,\n",
    "    'F': 5,  'G': 6,  'H': 7,  'I': 8,  'J': 21,\n",
    "    'K': 9,  'L': 10, 'M': 11, 'N': 12, 'O': 21,\n",
    "    'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17,\n",
    "    'U': 21, 'V': 18, 'W': 19, 'X': 21, 'Y': 20,\n",
    "    '-': 21  # Gap symbol\n",
    "}\n",
    "letter_to_num = {\n",
    "    'A': 0,  'B': 20, 'C': 1,  'D': 2,  'E': 3,\n",
    "    'F': 4,  'G': 5,  'H': 6,  'I': 7,  'J': 20,\n",
    "    'K': 8,  'L': 9, 'M': 10, 'N': 11, 'O': 20,\n",
    "    'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16,\n",
    "    'U': 20, 'V': 17, 'W': 18, 'X': 20, 'Y': 19,\n",
    "    '-': 20  # Gap symbol\n",
    "}\n",
    "\n",
    "letter_to_num_conversion = {\n",
    "    'A': 0,  'C': 1,  'D': 2,  'E': 3,\n",
    "    'F': 4,  'G': 5,  'H': 6,  'I': 7,  \n",
    "    'K': 8,  'L': 9, 'M': 10, 'N': 11, \n",
    "    'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16,\n",
    "    'V': 17, 'W': 18,  'Y': 19,\n",
    "    '-': 20  # Gap symbol\n",
    "}\n",
    "\n",
    "#switched the indexes to minus 1\n",
    "def prob_cond_unnormalized(aa,prev_aa,J_ten,beta=1):\n",
    "    i=len(prev_aa)\n",
    "    sum=0\n",
    "    for j in range(i):\n",
    "        sum+=J_ten[aa,prev_aa[j],i,j]\n",
    "    return np.exp(beta*sum)\n",
    "def Z_i_prob(aa_pot,prev_aa,J_tens):\n",
    "    list_prob=[prob_cond_unnormalized(aa,prev_aa,J_tens) for aa in aa_pot ]\n",
    "    return list_prob\n",
    "def sample(aa_pot,Z_list):\n",
    "    Z=np.sum(Z_list)\n",
    "    Z_list=Z_list/Z\n",
    "    return np.random.choice(aa_pot,p=Z_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_num_ale = {\n",
    "    'A': 1,  'B': 21, 'C': 2,  'D': 3,  'E': 4,\n",
    "    'F': 5,  'G': 6,  'H': 7,  'I': 8,  'J': 21,\n",
    "    'K': 9,  'L': 10, 'M': 11, 'N': 12, 'O': 21,\n",
    "    'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17,\n",
    "    'U': 21, 'V': 18, 'W': 19, 'X': 21, 'Y': 20,\n",
    "    '-': 21  # Gap symbol\n",
    "}\n",
    "letter_to_num = {\n",
    "    'A': 0,  'B': 20, 'C': 1,  'D': 2,  'E': 3,\n",
    "    'F': 4,  'G': 5,  'H': 6,  'I': 7,  'J': 20,\n",
    "    'K': 8,  'L': 9, 'M': 10, 'N': 11, 'O': 20,\n",
    "    'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16,\n",
    "    'U': 20, 'V': 17, 'W': 18, 'X': 20, 'Y': 19,\n",
    "    '-': 20  # Gap symbol\n",
    "}\n",
    "# switched here as well for the computation of the prob\n",
    "def initial_sample(L):\n",
    "    list_nb=np.arange(22)\n",
    "    init_sample=np.random.choice(list_nb,L)\n",
    "    return init_sample\n",
    "\n",
    "def plm_seq(seq,J):\n",
    "    sum=0\n",
    "    L=J.shape[-1]\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            sum+=J[seq[i],seq[j],i,j]\n",
    "    return sum\n",
    "\n",
    "def plm_aa_calc_diff(aa_new,ind_change,seq,J_tens):\n",
    "    L=J_tens.shape[-1]\n",
    "    sum=0\n",
    "    for j in range(L):\n",
    "        delta_J1=J_tens[aa_new,seq[j],ind_change,j]-J_tens[seq[ind_change],seq[j],ind_change,j]\n",
    "        delta_J2=J_tens[seq[j],aa_new,j,ind_change]-J_tens[seq[j],seq[ind_change],j,ind_change]\n",
    "        sum+=delta_J1+delta_J2\n",
    "    return sum\n",
    "\n",
    "def plm_aa_calc_diff_alter(aa_new,ind_change,seq,J_tens):\n",
    "    L=J_tens.shape[-1]\n",
    "    sum=0\n",
    "    for j in range(L):\n",
    "        delta_J1=J_tens[aa_new,seq[j],ind_change,j]\n",
    "        delta_J2=J_tens[seq[j],aa_new,j,ind_change]\n",
    "        sum+=delta_J1+delta_J2\n",
    "    return sum\n",
    "\n",
    "\n",
    "def plm_ind_change(aa_pot,ind,seq,J,beta=1,old_plm=None,test=False):#probablement false\n",
    "    if old_plm==None:\n",
    "        old_plm=plm_seq(seq,J)\n",
    "    list_aa_plm=np.array([])\n",
    "    list_aa_plm_alter=np.array([])\n",
    "    \n",
    "    for i in range(len(aa_pot)):\n",
    "        list_aa_plm=np.append(list_aa_plm,plm_aa_calc_diff(aa_pot[i],ind,seq,J))\n",
    "        list_aa_plm_alter=np.append(list_aa_plm_alter,plm_aa_calc_diff_alter(aa_pot[i],ind,seq,J))\n",
    "        # print(\"diff for two sequences\",plm_aa_calc_diff(aa_pot[i],ind,seq,J))\n",
    "        \n",
    "        if test==True:\n",
    "            seq_t=seq.copy()\n",
    "            seq_t[ind]=aa_pot[i]\n",
    "            print(\"diff two methods:\",old_plm+plm_aa_calc_diff(aa_pot[i],ind,seq,J)-plm_seq(seq_t,J))\n",
    "    prob_unn=np.exp(beta*list_aa_plm)    #à checker signe dans l'exponentiel\n",
    "    pro_alter=np.exp(beta*list_aa_plm_alter)\n",
    "    def assert_no_nan(arr, name=\"array\"):\n",
    "        \"\"\"\n",
    "        Checks if a NumPy array contains any NaNs.\n",
    "        If it does, raises a ValueError with debug info.\n",
    "        \n",
    "        Parameters:\n",
    "            arr (np.ndarray): Array to check.\n",
    "            name (str): Optional name to include in the error message.\n",
    "        \"\"\"\n",
    "        if np.isnan(arr).any():\n",
    "            nan_indices = np.argwhere(np.isnan(arr))\n",
    "            sample = arr.flatten()[np.isnan(arr.flatten())][:5]  # First 5 NaNs\n",
    "            raise ValueError(\n",
    "                f\"NaN detected in {name}!\\n\"\n",
    "                f\"First few NaN values: {sample}\\n\"\n",
    "                f\"Indices of NaNs: {nan_indices[:5]}\\n\"\n",
    "                f\"Total NaNs: {np.isnan(arr).sum()}\\n\"\n",
    "                f\"old plm: {old_plm}\\n\"\n",
    "                f\"prob_list: {prob_unn}\"\n",
    "            )\n",
    "    assert_no_nan(prob_unn/np.sum(prob_unn))\n",
    "    print(\"difference of proba(should be around zero):\",prob_unn/np.sum(prob_unn)-pro_alter/np.sum(pro_alter))\n",
    "    return prob_unn/np.sum(prob_unn), list_aa_plm\n",
    "\n",
    "def plm_ind_change_quick(aa_pot,ind,seq,J,beta=1):\n",
    "    list_aa_plm=np.zeros(len(aa_pot))\n",
    "    #list_aa_plm=np.array([])\n",
    "    for i in range(len(aa_pot)):\n",
    "        #list_aa_plm=np.append(list_aa_plm,plm_aa_calc_diff_alter(aa_pot[i],ind,seq,J))\n",
    "        list_aa_plm[i]=plm_aa_calc_diff_alter(aa_pot[i],ind,seq,J)\n",
    "    prob_unn=np.exp(beta*list_aa_plm) \n",
    "    return prob_unn/np.sum(prob_unn)\n",
    "\n",
    "def plm_sample(aa_pot,proba_list):\n",
    "    ind_choice=np.random.choice(range(len(aa_pot)),p=proba_list)\n",
    "    return aa_pot[ind_choice],ind_choice\n",
    "\n",
    "\n",
    "def generate_plm(J_tens,maxiter=10000,beta=1,initial_seq=None,quick=True):\n",
    "    L=J_tens.shape[-1]\n",
    "    if initial_seq is None:\n",
    "        sequence=initial_sample(L)\n",
    "    else:\n",
    "        sequence=initial_seq\n",
    "    list_nb=np.arange(21)\n",
    "    seq_list=[]\n",
    "    seq_list.append(sequence.copy())\n",
    "    position_list=np.arange(L)\n",
    "    ind_change_list=np.random.choice(position_list,size=maxiter)\n",
    "    if quick:\n",
    "        for i in tqdm(range(maxiter)):\n",
    "            ind_change=ind_change_list[i]\n",
    "            prob_list=plm_ind_change_quick(list_nb,ind_change,sequence,J_tens,beta=beta)\n",
    "            new_aa,ind_chosen=plm_sample(list_nb,prob_list)\n",
    "            sequence[ind_change]=new_aa\n",
    "            seq_list.append(sequence.copy())\n",
    "        return np.array(seq_list)\n",
    "    else:\n",
    "        old_plm=plm_seq(sequence,J_tens)\n",
    "        plm_list=[]\n",
    "        plm_list.append(old_plm)\n",
    "        for i in tqdm(range(maxiter)):\n",
    "            old_sequence=sequence.copy()\n",
    "            ind_change=np.random.choice(np.arange(L))\n",
    "            prob_list,pot_plm=plm_ind_change(list_nb,ind_change,sequence,J_tens,beta=beta,old_plm=old_plm)\n",
    "            new_aa,ind_chosen=plm_sample(list_nb,prob_list)\n",
    "            sequence[ind_change]=new_aa\n",
    "            seq_list.append(sequence.copy())\n",
    "            old_plm=pot_plm[ind_chosen]\n",
    "            #print(\"difference in plm calc method:\",(old_plm-plm_seq(sequence,J_tens))/old_plm) #seems close enough but not in the computer numerical errors range \n",
    "            plm_list.append(pot_plm[ind_chosen])\n",
    "        return np.array(seq_list),plm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 19 19  1 17  9  5 17 10  8  2  0  2  0  8 15  7  8  8  0  4 14  8  9\n",
      "  0 14 15 19  6  6  2 17 11 12  5  2  8  3  0  3 14  8  4  8  3  1 11  3\n",
      "  0 11  3 17  9 15  2 12  3 20 14  8  8 19  2]\n",
      "DYYCVLGVMKDADAKSIKKAFRKLARSYHHDVNPGDKEAERKFKECNEANEVLSDPE-RKKYD\n",
      "DYYCVLGVMKDADAKSIKKAFRKLARSYHHDVNPGDKEAERKFKECNEANEVLSDPE-RKKYD\n"
     ]
    }
   ],
   "source": [
    "def nums_to_letters(sequence):\n",
    "    \"\"\"Receives a sequence of integers (e.g., [1, 2, 4]) and returns a string of corresponding amino acid letters.\"\"\"\n",
    "    num_to_letter = {v: k for k, v in letter_to_num.items()}\n",
    "    return ''.join([num_to_letter.get(num, 'X') for num in sequence])\n",
    "\n",
    "init_seq = 'DYYQVLGVPKDADAKSIKKAFRKLARKYHPDVNPGDKEAERKFKEANEANEVLSDPEKRKKYD'\n",
    "init_sequence_num = letters_to_nums(init_seq)\n",
    "ratio = 0.1\n",
    "init_sequence_num = modify_seq(init_sequence_num, ratio)\n",
    "print(init_sequence_num)\n",
    "print(nums_to_letters(init_sequence_num))\n",
    "inv_letter_to_num_conversion = invert_dict(letter_to_num_conversion)\n",
    "print(numbers_to_letters(init_sequence_num, inv_letter_to_num_conversion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 35.54it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "non-string returned while reading data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m gen_sequences_y \u001b[38;5;241m=\u001b[39m generate_plm(Jtens,maxiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, initial_seq\u001b[38;5;241m=\u001b[39minit_sequence_num\u001b[38;5;241m.\u001b[39mcopy() ,quick\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(gen_sequences_y), \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Convert the sequence at index i to letters and print\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     sequence_in_letters \u001b[38;5;241m=\u001b[39m \u001b[43mseq_num_to_letters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_sequences_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mletter_to_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (in letters): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_in_letters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m, in \u001b[0;36mseq_num_to_letters\u001b[0;34m(file_path, dictionary)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mseq_num_to_letters\u001b[39m(file_path,dictionary):\n\u001b[1;32m     56\u001b[0m     inv_dict\u001b[38;5;241m=\u001b[39minvert_dict(dictionary)\n\u001b[0;32m---> 57\u001b[0m     seq_array\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m     58\u001b[0m     letter_seq_array\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m seq_array:\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_env/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1397\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1395\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1397\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/miniconda3/envs/bio_env/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1036\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1036\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[0;31mTypeError\u001b[0m: non-string returned while reading data"
     ]
    }
   ],
   "source": [
    "gen_sequences_y = generate_plm(Jtens,maxiter=1000,beta = 1, initial_seq=init_sequence_num.copy() ,quick=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'results_plm'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "file_path = os.path.join(folder_name, 'generated_sequences.txt')\n",
    "np.savetxt(file_path, gen_sequences_y,fmt=\"%.5f\")\n",
    "seq_num_to_letters(file_path,letter_to_num_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 (in letters): DYYQVLGVKKDHDAKSIKKAFRKLARKYHPDVNPGDNEAERKFKEAREANEVLSDKEKRKKYD\n",
      "Sequence 101 (in letters): ARYDVKALAVDHDQPSLLEALHELALKLRDDYRAGDRFATAVYREAQADTLKLQEYHAHKHRD\n",
      "Sequence 201 (in letters): ARYALLAEAVDHDLLSLLAAKHVLALQLRDAYRADYRDAAAVDRELQADTLRLQDYHHLIFRA\n",
      "Sequence 301 (in letters): ARAELRAAAVDDLLASLLAAGRAKALTLRDAYKADLRDEAFRQRERRAATLELRDPAAQIFRA\n",
      "Sequence 401 (in letters): ARDELRADAVADLRAAVAAALRAPALYLADAATAALRPAAFRDRARLAAALLLRDPAAQRARA\n",
      "Sequence 501 (in letters): ARDELRADAAADAQAAVAAAGRADLAALADAAAAAVRPAAARDRARLAAALLARDPAAERARA\n",
      "Sequence 601 (in letters): ARATVAADLAAAAAAAVAAALRDDAAALADAAAAAVRPAAARDRARLAAAAAARAPAAERARA\n",
      "Sequence 701 (in letters): ARLTLAADAAAAAAAAAAALGRLTAAALADAAARALRRAAARDAARAAAAAAAAAPTAARARA\n",
      "Sequence 801 (in letters): ARLALAADAAAAAAAAAQALLAATAAARADDAAVALRFAAARDAARRAAAAAAAAPAAARARA\n",
      "Sequence 901 (in letters): ARTALAADLAAAAAAAAAALTAVTAAAAADAAAVALRRAAARDAARRAAAAAAAAPAAAEARA\n",
      "Sequence 1001 (in letters): ARTALAADLAAAAAAAAPAAPAATAAAAADAAAVALRLAAAADAARRAAAAVARAPAAAAARA\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(gen_sequences_y), 100):\n",
    "    # Convert the sequence at index i to letters and print\n",
    "    sequence_in_letters = nums_to_letters(gen_sequences_y[i])\n",
    "    print(f\"Sequence {i + 1} (in letters): {sequence_in_letters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
